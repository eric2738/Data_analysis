{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 제목으로 다음 뉴스에서 검색하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 : 도지코인\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장할 파일명을 입력하세요 :dozi\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "# 네이버 기사 제목, 발행날짜\n",
    "\n",
    "\n",
    "def get_news(query):\n",
    "    news_df = pd.DataFrame(columns=(\"Title\", \"Datetime\"))\n",
    "    idx = 0\n",
    "\n",
    "    # url_query = quote(query)\n",
    "    for i in (range(1, 11, 10)):\n",
    "        url = 'https://search.naver.com/search.naver?where=news&sort=1&query={}&start={}'.format(\n",
    "            query, i)\n",
    "        res = requests.get(url)\n",
    "        res.raise_for_status()    # 문제생기면 종료\n",
    "\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        cont = soup.find_all('div', {'class': 'news_area'})\n",
    "\n",
    "        for i in cont:\n",
    "            title = i.find('a', {'class': 'news_tit'}).text\n",
    "            datetime = i.find('span', {'class': 'info'}).get_text()\n",
    "\n",
    "            news_df.loc[idx] = [title, datetime]\n",
    "            idx += 1\n",
    "\n",
    "    return news_df\n",
    "\n",
    "\n",
    "# 검색\n",
    "query = input('검색어 : ')\n",
    "news_df = get_news(query)\n",
    "# news_df\n",
    "\n",
    "# 기사제목으로 다음에서 검색\n",
    "idx = 0\n",
    "dnews_df = pd.DataFrame(columns=(\"Title\", \"Datetime\", \"Url\", \"Article\"))\n",
    "for n_title in tqdm(news_df['Title']):\n",
    "    url = 'https://search.daum.net/search?w=news&q=' + n_title\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    cont = soup.find('div', {'class': 'cont_inner'})\n",
    "    if cont == None:\n",
    "        title = news_df['Title'][0]\n",
    "        datetime = 'None'\n",
    "        art_href = 'None'\n",
    "        href_cont = 'None'\n",
    "    else:\n",
    "        title = cont.find('a', {'class': 'f_link_b'}).text\n",
    "        datetime = cont.find('span', {'class': 'f_nb date'}).text\n",
    "\n",
    "    # href가 <a class=\"f_link_b\" href=\"http://v.media.daum.net/v/20210227100223206?f=o\" 이렇게 나옴. \n",
    "    # => 다음뉴스페이지가 아니라 오리지널신문사 페이지로 넘어감\n",
    "    # 뒤에 숫자만 따서 'https://news.v.daum.net/v/' 여기에 붙여줌\n",
    "    # 숫자 스플릿\n",
    "        art_url = cont.find('a', {'class': 'f_link_b'})['href']\n",
    "\n",
    "        if 'cp' in art_url:\n",
    "            art_href = art_url\n",
    "            href_cont = 'None'\n",
    "        else:\n",
    "            art_href = 'https://news.v.daum.net/v/' + \\\n",
    "                art_url.split('/')[4].split('?')[0]\n",
    "            # art_href에 requests\n",
    "            href_res = requests.get(art_href)\n",
    "            href_soup = BeautifulSoup(href_res.text, 'html.parser')\n",
    "            href_cont = href_soup.select_one(\n",
    "                'div#harmonyContainer').text.replace('\\n', '').strip()\n",
    "            href_cont\n",
    "\n",
    "    # dataframe으로 만들어서 엑셀파일 생성\n",
    "\n",
    "    dnews_df.loc[idx] = [title, datetime, art_href, href_cont]\n",
    "    idx += 1\n",
    "    time.sleep(1)\n",
    "\n",
    "name = input(\"저장할 파일명을 입력하세요 :\")\n",
    "xlsx = \".xlsx\"\n",
    "filename = name+xlsx\n",
    "dnews_df.to_excel('./'+filename, index=False, encoding='utf-8')\n",
    "\n",
    "# 문제 1 : 다음뉴스로 넘어가는 링크가 없는 기사 = 오리지널 링크만 있는 기사 는 href가 다름\n",
    "# 문제 2 : 네이버 기사가 다음 기사에 없는 경우\n",
    "\n",
    "# 문제 1 처리 :\n",
    "# 문제 2 처리 : soup.find('div',{'class':'cont_inner'}) 이 None인 경우 title만 저장.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제점\n",
    "- 다음뉴스가 아니면 크롤링을 못함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
